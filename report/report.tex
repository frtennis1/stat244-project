%        File: proposal.tex Created: Wed Nov 07 07:00 PM 2018 E Last Change: Wed
%     Nov 07 07:00 PM 2018 E
%
\documentclass[letterpaper, 12pt]{article}
\usepackage{bbm}
% \usepackage{hyperref
\usepackage[thm]{macros}
% \usepackage{natbib}
% \usepackage{nopageno}
\doublespacing
\newgeometry{margin=1in}

\pagestyle{fancy}
\lhead{}
\chead{\textsc{High-dimensional GLMs}}
\rhead{}

\newcommand{\by}{\bm y}
\newcommand{\bx}{\bm x}
\newcommand{\bX}{\bm X}
\newcommand{\bW}{\bm W}
\newcommand{\stb}{\bm e}
\newcommand{\bbeta}{\bm \beta}
\newcommand{\one}{\mathbbm{1}}
\newcommand{\trans}{\intercal}
\newcommand{\bg}{\bm g}
\newcommand{\bgamma}{\bm \gamma}
\begin{document}

\title{\textbf{Model Selection and Estimation in High-dimensional Generalized Linear Models}}
\author{Francisco Rivera \and Jiafeng (Kevin) Chen}
\date{\today}

\maketitle

\section{Introduction}
The workhorse for high-dimensional regressions is the $\ell_1$ lasso penalty
\citep{tibshirani1996regression}. The original lasso is developed for fitting
(normal) linear models with the objective\footnote{We consider $\by$ an $n$-vector of response variables whose $i$th element is $\by_i$. We consider the covariate matrix $\bX$ ($n \times p$). In most settings, $\bX$ include an intercept column, $\bx_{0} = \bm 1$, that is not regularized.} \begin{equation}
    \hat \bbeta_{\text{lasso}} = \argmin_{\bbeta}\, \norm{\by - \bX\bbeta}^2 + \lambda \norm{\bbeta}_1 - \lambda |\beta_0|, \, \lambda > 0.
    \label{eq:linear_lasso}
\end{equation}
The first paper about lasso in a GLM setting is \cite{park2007l1}: Consider a scalar GLM with likelihood \[
L(y; \theta,\phi) = \exp\br{\frac{y\theta - b(\theta)}{a(\phi)} + c(y, \phi)},
\]
where $g(\E[y]) = \eta = \bx^\trans \bbeta$ for some scalar function $g$. Thus we may consider a direct extension of \eqref{eq:linear_lasso}: \begin{equation}
    \hat \bbeta_{\text{GLM-lasso}} = \argmin_{\bbeta}\, \underbrace{-\frac{1}{n}\sum_{i=1}^n \bk{y_i \theta(\bbeta)_i - b\pr{\theta(\bbeta)_i}}}_{\ell_n(\bbeta)} + \lambda \norm{\bbeta}_1 - \lambda |\beta_0|,
    \label{eq:glm_lasso}
\end{equation}
where \eqref{eq:glm_lasso} reduces to \eqref{eq:linear_lasso} if the likelihood is Gaussian and $g$ is the canonical link for the Gaussian model, which is the identity function. Note that the objective \eqref{eq:glm_lasso} is convex if we use the canonical link, since the exponential family log-likelihood is concave in $\theta$, and $\theta = \bx^\trans \bbeta$ is linear in $\bbeta$ for the canonical link.

\section{Model Selection with Lasso}

Brute-force model selection scales poorly with respect to the number of
predictors. That is, when building a model to predict $\bm{y}$ from a subset of
the columns of $\bX \in \mathbb{R}^{n,p}$ (since not all columns may be
predictive), there are $2^p$ subsets of columns to regress on. Checking all of
these models against each other incurs a cost exponential in $p$ and becomes
prohibitively expensive in a high-dimensional setting.

In class, we tackled this problem with forward-selection and backward-deletion,
a greedy algorithm that adds and removes covariates until a local optimal is
reached. \cite{park2007l1}'s use of the lasso for GLMs provides a competitive
alternative that behaves less greedily. The high-level idea is to note the
non-zero coefficients for different penalizations $\lambda$ and constrain our
search to these models. We describe their procedure in this section.

In order to employ this method, we require a method to solve
\eqref{eq:glm_lasso}. For now, we suppose that we have such a method and can use
it as a black box. It is sufficient for us to know that this black box takes in
an initial $\bbeta^\text{init}$ and employs a descent-based method to converge
toward the optimal $\bbeta^*$. \cref{sec:estimation} follows-up by describing
the inner workings of different approaches.

The lasso penalty induces a sparsity in our optimal $\bbeta$. This sparsity
increases as our penalization term $\lambda$ becomes bigger. Indeed, if we take
$\lambda \to \infty$, then our solution is driven to $\bbeta \to \bm{0}$ because
the penalization term is all that matters, and the norm of $\bbeta$ is minimized
at $\bm{0}$. The entire domain when $\bbeta = \bm{0}$ is uninteresting, so we
initialize our algorithm at $\lambda_\text{max}$ which we define as the smallest
$\lambda$ such that there is only one non-zero coefficient.

Once we have initialize $\lambda$ in our algorithm to $\lambda_\text{max}$, we
will repeat three steps that \cite{park2007l1} outline,

\begin{enumerate}

\item Determine the step length by which to decrease $\lambda$. The intent is to
pick this such that the precisely one more coefficient becomes non-zero.
Introducing notation, we let $\lambda_k$ be the $\lambda$ value we consider in
the $k$th step, and we have to pick $\lambda_k - \lambda_{k+1}$.

\item Using a linear approximation, predict the value of $\bbeta$ for the
decremented $\lambda_{k+1}$.

\item Initialize our solver of \eqref{eq:glm_lasso} with the linear estimate as
$\bbeta^\text{init}$ and get the precise solution $\bbeta^*$ for the new
$\lambda$.

\end{enumerate}

doing so until either we have brought $\lambda$ all the way to 0, or we
determine that incremental variables are not improving the model. In order to
perform the second step, a linear approximation prescribes that,

\[ \bbeta_{k+1} \approx \bbeta_k + (\lambda_{k+1} - \lambda_k) \frac{\partial
\bbeta}{\partial \lambda}.\]

Denote by $\bX_A$ the matrix made up by the columns of $\bX$ whose corresponding
$\bbeta$ coefficient is non-zero, i.e. in the active set. Furthermore, let $\bW$
be the diagonal matrix with $i^\text{th}$ diagonal element given by,

\[ \frac{1}{\var(y_i)} \left( \frac{\partial \mu}{\partial \eta} \right)^2\]

which changes because the derivative is evaluated at a different location based
on the value of $\lambda$, so we define $\bW_k$ as the value of $\bW$
corresponding to $\lambda_k$. Then, we have that,

\[ \bbeta_{k+1} \approx \bbeta_k - (\lambda_{k+1} - \lambda_k) (\bW_A^\top \bW_k
\bX_A)^{-1} \sgn (\beta_k).\]

Note that in the above equation, we pretend $\bbeta_k$ and $\bbeta_{k+1}$ only
contain their non-zero components, and thus are not going to have all $p$
dimensions. This makes our algebra simpler and we can do this because the linear
approximation will not make a previously zero coefficient non-zero.

Finally, we just need to determine our step size. One simple choice could be to
use a constant step size. However, in different domains of $\lambda$, the same
unit change in $\lambda$ could have dramatically different implications for the
sparsity of the resulting $\bbeta$. If we were working with a lasso in linear
regression, $\bW$ would not change, and we could pick the step size precisely to
coincide with the point at which a coefficient of zero becomes non-zero.
\cite{park2007l1} suggest doing precisely this, with the caveat that because
$\bW$ does change, it will only be an approximation for the GLM case. 


\section{Estimation}
\label{sec:estimation}

\cite{efron2004least} gives an efficient estimation procedure for the linear
lasso \eqref{eq:linear_lasso} called the Least Angle Regression (LAR), which
relies on the fact that the \emph{regularization path}---$\hat \bbeta_i$ as a
function of $\lambda$---is piecewise linear in \eqref{eq:linear_lasso}. Such a
structure is often unavailable in applications like \eqref{eq:glm_lasso}. The
most popular method---proposed by \cite{friedman2010regularization} and
implemented in \textsf{R}'s \textsf{glmnet} package---is \emph{cyclical
coordinate descent} with iteratively reweighted least squares. The idea is to
approximate $\ell_n(\bbeta)$ in \eqref{eq:glm_lasso} with a second-order Taylor
expansion, either globally for all parameters $\bbeta$ (for scalar-valued GLMs)
or locally with a single parameter $\bbeta_j$ (for vector-valued GLMs, such as
the multinomial logistic regression). Such an approximation yields a quadratic
function (in the scalar GLM case) \[
\ell_{Q}(\bbeta) = \frac{1}{n}\sum_{i=1}^n w_{i} (y_i - \bx_i^\trans \bbeta)^2.
\]
We then solve a local penalized least-squares problem:\begin{equation}
    \bbeta \gets \argmin_{\bbeta} \ell_{Q}(\bbeta) + \lambda \norm{\bbeta}_1.
    \label{eq:local_opt}
\end{equation}
via cyclical coordinate descent, i.e. by iteratively solving \begin{equation}
    \bbeta_j \gets \argmin_{\bbeta_j} \ell_{Q}(\bbeta) + \lambda \norm{\bbeta}_1,
    \label{eq:cyclic_opt}
\end{equation}
holding all other entries $\bbeta_{-j}$ fixed. \eqref{eq:cyclic_opt} has an analytical solution for the lasso penalty\footnote{\cite{friedman2010regularization} show a similar expression for the \emph{elastic net} penalty: \[
\lambda P_\alpha(\bbeta) = \lambda \pr{\alpha \norm{\bbeta}_1 + (1-\alpha) \norm{\bbeta}_2}.
\]}
\begin{equation}
    \bbeta_j \gets \frac{S\pr{\sum_{i=1}^N w_i x_{ij}\pr{y_i - \tilde y_i\uppr{j}}, \lambda}}{\sum_{i=1}^N w_i x_{ij}^2}, \, S(t, \gamma) = \sgn(t)\pr{|z|-\gamma}_+,\, \tilde y_i\uppr{j} = \bx_i^\trans \bbeta - x_{ij}\bbeta_j.
    \label{eq:ccd_update}
\end{equation}
We summarize the procedure described above in \Cref{alg:ccd}. 
\begin{algorithm}
\caption{Cyclic coordinate descent algorithm for solving \eqref{eq:glm_lasso} (scalar GLM case) in \cite{friedman2010regularization}}
\label{alg:ccd}
\begin{algorithmic}
\State{Initialize $\bbeta$}
\For{$\lambda$ on regularization path}
\While{$\bbeta$ has not converged}
\State{Approximate $\ell_n(\beta)$ by $\ell_Q(\beta)$}
\While{cyclical descent has not converged}
\For{$j$}
    \State{Update $\bbeta_j$ according to \eqref{eq:ccd_update}}
\EndFor
\EndWhile
\EndWhile
\State{$\hat{\bbeta}_\lambda \gets \bbeta$}
\State{Initialize $\bbeta$ for next iteration to $\hat{\bbeta}_\lambda$}
\EndFor
\end{algorithmic}
\end{algorithm}

The machine learning literature slightly alters \Cref{alg:ccd} and changes \eqref{eq:ccd_update} into \[
\bbeta_j \gets S\pr{\bbeta_j - \pr{\nabla_{\bbeta} \ell_n(\bbeta)}_j \kappa^{-1}, \frac{\lambda}{\kappa}}
\]
for some \emph{learning rate} $1/\kappa$, in keeping with gradient descent. Moreover, \cite{shalev2011stochastic} proves a convergence guarantee for stochastic coordinate descent in this fashion, where, instead of cycling through the coordinates of $\bbeta$, a coordinate is chosen uniformly at random. 
\begin{theorem}
Let $Q(\bbeta)$ be the objective in \eqref{eq:glm_lasso}. 
    At iteration $T$ of the first while-loop in a verison of \Cref{alg:ccd} with stochastic coordinate descent and gradient updates, \[
    \E[Q(\bbeta_T)] - \E[Q(\hat\bbeta_{\text{GLM-lasso}})] \le C\frac{p \kappa}{T+1}
    \]
    for constant $C$ a function of the initial starting value $\bbeta\uppr{0}$, assuming that $\ell_n$ is differentiable with \[
    \ell_n (\bbeta + \eta \stb_j) \le \ell_n(\bbeta) + \eta \pr{\nabla \ell_n}_j + \frac{\kappa}{2} \eta^2
    \]
    for all $\eta, \bbeta, j$.\footnote{This condition restricts the choice of $\kappa$ as a function of the loss criterion.}
\end{theorem}
\begin{cor}
    The runtime to achieve $\epsilon$ expected accuracy is bounded by \[O\pr{\frac{np\kappa }{\epsilon} \norm{\hat \bbeta_{\text{GLM-lasso}}}_2^2}.\]
\end{cor}

Moreover, \cite{bradley2011parallel} show that a parallel version of the
coordinate gradient descent procedure above where at each iteration, $\mathsf P$
(possibly duplicate) coordinates are updated in parallel. For correlated
features, such parallelism is dangerous, since updating two correlated features
simulataneously may over or undercompensate for the gradient direction.
\cite{bradley2011parallel} quantifies the interference due to correlated
features and shows that efficiency increases linearly in the number of parallel
processes $\mathsf{P}$ so long as $\mathsf{P} \le \frac{p}{\rho}$ where $\rho$
is the largest modulus of the eigenvalues of $X^\trans X$.

Coordinate descent methods described above can also become expensive if $n$ is
large. The standard machine learning and optimization answer to this problem is
to use \emph{stochastic gradient descent}, replacing $\nabla_{\bbeta}
\ell_n(\bbeta)$ with an unbiased estimate $\bm g_i = \nabla_{\bbeta} \log L(y_i;
\bbeta)$, which is the gradient evaluated on a single observation.\footnote{We
can replace this with \emph{batched gradient descent} as well, where the
gradient estimate is averaging over a batch of observations.}
\cite{shalev2011stochastic} consider a mirror descent algorithm in the lasso
context, by running stochastic gradient descent on the dual problem and
enforcing sparsity in an intelligent manner. Let $\bgamma = f(\bbeta)$ be the
dual parameter for $\bbeta$ with an invertible link $f$. We choose an
observation $i$ at random, compute $\bg_i$, and update \begin{align*}
\bgamma &\gets \bgamma - \eta \bg_i \\
\bgamma' &\gets \bgamma - \eta\lambda \sgn(\bgamma) \tag{Decrease $\norm{\bbeta}_1$}\\
\bgamma_j &\gets \bgamma'_j \one\pr{\sgn(\bgamma_j) = \sgn(\bgamma'_j)} \tag{Maintains sparsity} \\
\bbeta &\gets f^{-1}(\bgamma).
\end{align*}
The runtime bound for the stochastic mirror descent algorithm in
\cite{shalev2011stochastic} is \[O\pr{\frac{p\log p}{\epsilon^2} \norm{\hat
\bbeta_{\text{GLM-lasso}}}_2^2}.\] We pay the price of the $p\log p$ and
$\epsilon^{-2}$ dependence, as opposed to $p$ and $\epsilon^{-1}$, in order to
achieve the benefit of a $n$-free runtime.


\newpage
\nocite{*}
\bibliographystyle{jpe}
\bibliography{sources}

\end{document}


